"""
ğŸ“˜ qlib_genrate_datasets.py
-------------------------------------------------------
ä¾æ“š Incremental Pretraining / Online Fine-tuning éšæ®µï¼Œ
è®€å–è‚¡ç¥¨æ± å®šç¾©æª” (CSV)ï¼Œ
è‡ªå‹•ç”¢ç”Ÿè¨“ç·´æˆ–æ¸¬è©¦è³‡æ–™é›†ï¼ˆQlib å¯ç”¨æ ¼å¼ï¼‰ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
-------------------------------------------------------
Incremental Pretrainingï¼š
python ./src/qlib_generate_datasets.py --mode incremental_pretrain --year 1992

Online Fine-tuningï¼š
# python ./src/qlib_generate_datasets.py --mode online_ft --date 1993-01-05
python ./src/qlib_generate_datasets.py --mode online_ft --year 1993
-------------------------------------------------------
"""

import argparse
import pandas as pd
from pathlib import Path
from qlib.data import D
from qlib.contrib.data.handler import Alpha158
from qlib.data.dataset import DatasetH
import qlib

# === åˆå§‹åŒ– Qlib ===
qlib.init(provider_uri="./data/qlib_data/day1", region="cn", num_workers=8)
print("âœ… Qlib Initialized")

# === åƒæ•¸è¨­å®š ===
parser = argparse.ArgumentParser()
parser.add_argument("--mode", choices=["incremental_pretrain", "online_ft"], required=True)
parser.add_argument("--year", type=int, help="Incremental pretraining year (e.g. 1992)")
parser.add_argument("--date", type=str, help="Online fine-tuning date (e.g. 1993-01-05)")
args = parser.parse_args()

# === è·¯å¾‘è¨­å®š ===
BASE_DIR = Path("./data/stock_pools")
PRETRAIN_DIR = BASE_DIR / "incremental_pretrain"
ONLINE_FT_DIR = BASE_DIR / "online_ft"
OUTPUT_DIR = Path("./data/qlib_data/day1/generated_datasets")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# === å…±ç”¨å‡½å¼ï¼šç”¢ç”Ÿ Alpha158 ç‰¹å¾µ ===
def generate_alpha158_features(symbols, start_time, end_time, fit_start_time, fit_end_time, output_path):
    """ç”¢ç”Ÿ Alpha158 ç‰¹å¾µä¸¦è¼¸å‡º CSV"""
    print(f"ğŸ§® Generating Alpha158 features for {len(symbols)} stocks: {start_time} â†’ {end_time}")

    handler = Alpha158(
        instruments=symbols,
        start_time=start_time,
        end_time=end_time,
        fit_start_time=fit_start_time,
        fit_end_time=fit_end_time,
    )

    # dataset = DatasetH(handler, segments={"full": (start_time, end_time)})

    # å–å¾—ç‰¹å¾µåç¨±
    ohlcv_fields = ["$open", "$high", "$low", "$close", "$volume"]
    ohlcv_names = ["open", "high", "low", "close", "volume"]
    feature_expressions, feature_names = handler.get_feature_config()
    df = D.features(symbols, ohlcv_fields + feature_expressions, start_time=start_time, end_time=end_time)
    
    if df.empty:
        print(f"âš ï¸ {output_path} ç‚ºç©ºï¼ˆå¯èƒ½è©²æ—¥ç„¡äº¤æ˜“æˆ– symbol ä¸ç¬¦ï¼‰")
    else:
        expected_cols = len(ohlcv_names) + len(feature_names)
        if df.shape[1] != expected_cols:
            print(f"âš ï¸ æ¬„ä½æ•¸é‡ä¸ç¬¦ï¼šé æœŸ {expected_cols}ï¼Œå¯¦éš› {df.shape[1]}")
        df.columns = ohlcv_names + feature_names


    # æ›æˆå¯è®€åç¨±
    df.columns = ohlcv_names + feature_names

    # å±•å¹³æˆæ¨™æº–è¡¨æ ¼
    df = df.reset_index().rename(columns={"datetime": "date", "instrument": "symbol"})

    df.to_csv(output_path, index=False)
    print(f"âœ… Saved Alpha158 dataset â†’ {output_path}")

# === æ¨¡å¼ Aï¼šIncremental Pretraining ===
if args.mode == "incremental_pretrain":
    # === åˆ†åˆ¥è®€å– train / valid è‚¡ç¥¨æ±  ===
    train_pool_path = BASE_DIR / f"incremental_pretrain/train/{args.year}.csv"
    valid_pool_path = BASE_DIR / f"incremental_pretrain/valid/{args.year}.csv"

    if not train_pool_path.exists() or not valid_pool_path.exists():
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°è‚¡ç¥¨æ± æª”æ¡ˆï¼š{train_pool_path} æˆ– {valid_pool_path}")

    train_pool = pd.read_csv(train_pool_path)
    valid_pool = pd.read_csv(valid_pool_path)
    print(f"ğŸ“˜ Loaded Incremental Pretrain TRAIN ({args.year}) â€” {len(train_pool)} stocks")
    print(f"ğŸ“˜ Loaded Incremental Pretrain VALID ({args.year}) â€” {len(valid_pool)} stocks")

    train_symbols = [Path(f).stem for f in train_pool["file"]]
    valid_symbols = [Path(f).stem for f in valid_pool["file"]]
    print(f"Train symbols: {train_symbols[:5]} ... Total: {len(train_symbols)}")
    print(f"Valid symbols: {valid_symbols[:5]} ... Total: {len(valid_symbols)}")

    # === å®šç¾©æ™‚é–“å€é–“ ===
    train_start = train_pool["start_date"].min()
    train_end = f"{args.year - 1}-12-31"
    valid_start = f"{args.year}-01-01"
    valid_end = f"{args.year}-12-31"

    # === ç”¢ç”Ÿ train/valid ç‰¹å¾µ ===
    train_out = OUTPUT_DIR / f"incremental_pretrain/train/{args.year}.csv"
    valid_out = OUTPUT_DIR / f"incremental_pretrain/valid/{args.year}.csv"
    train_out.parent.mkdir(parents=True, exist_ok=True)
    valid_out.parent.mkdir(parents=True, exist_ok=True)

    generate_alpha158_features(
        train_symbols,
        start_time=train_start,
        end_time=train_end,
        fit_start_time=train_start,
        fit_end_time=train_end,
        output_path=train_out
    )
    generate_alpha158_features(
        valid_symbols,
        start_time=valid_start,
        end_time=valid_end,
        fit_start_time=train_start,
        fit_end_time=train_end,
        output_path=valid_out
    )


# # === æ¨¡å¼ Bï¼šOnline Fine-tuning ===
# elif args.mode == "online_ft":
#     year = args.date[:4]
#     csv_path = ONLINE_FT_DIR / year / f"{args.date}.csv"

#     if not csv_path.exists():
#         raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°è‚¡ç¥¨æ± æª”æ¡ˆï¼š{csv_path}")
    
#     stock_pool = pd.read_csv(csv_path)
#     print(f"ğŸ“˜ Loaded Online FT Universe ({args.date}) â€” {len(stock_pool)} stocks")

#     symbols = [Path(f).stem for f in stock_pool["file"]]
#     print(f"Symbols: {symbols[:5]} ... Total: {len(symbols)}")

#     # å–®æ—¥ï¼ˆæˆ–å¯æ”¹æˆ rolling å¤šæ—¥ï¼‰
#     train_start = stock_pool["start_date"].min()
#     train_end = args.date
#     test_start = args.date
#     test_end = args.date

#     output_path = OUTPUT_DIR / f"online_ft/{year}/{args.date}.csv"
#     output_path.parent.mkdir(parents=True, exist_ok=True)

#     generate_alpha158_features(
#         symbols, 
#         start_time=test_start,
#         end_time=test_end,
#         fit_start_time=train_start,
#         fit_end_time=train_end, 
#         output_path=output_path
#     )

# === æ¨¡å¼ Bï¼šOnline Fine-tuningï¼ˆæ”¹ç‚ºè¼¸å…¥ yearï¼Œç”Ÿæˆè©²å¹´æ‰€æœ‰æ—¥æœŸï¼‰ ===
elif args.mode == "online_ft":

    if args.year is None:
        raise ValueError("âŒ online_ft æ¨¡å¼å¿…é ˆè¦æŒ‡å®š --yearï¼Œä¾‹å¦‚ --year 1993")

    year = args.year
    pool_dir = ONLINE_FT_DIR / str(year)

    if not pool_dir.exists():
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°è‚¡ç¥¨æ± è³‡æ–™å¤¾ï¼š{pool_dir}")

    # æ”¶é›†è©²å¹´åº¦æ‰€æœ‰æ± æ–‡ä»¶ï¼ˆæ¯å¤©ä¸€å€‹ CSVï¼‰
    csv_files = sorted(pool_dir.glob("*.csv"))
    if not csv_files:
        raise FileNotFoundError(f"âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•è‚¡ç¥¨æ±  CSVï¼š{pool_dir}")

    print(f"ğŸ“˜ Loaded Online FT Universe YEAR={year} â€” {len(csv_files)} days")

    # é€æ—¥ç”¢ç”Ÿ Alpha158 ç‰¹å¾µ
    for csv_path in csv_files:
        date_str = csv_path.stem   # 1993-01-05
        stock_pool = pd.read_csv(csv_path)

        symbols = [Path(f).stem for f in stock_pool["file"]]
        print(f"\nğŸ“… {date_str} â€” {len(symbols)} symbols")

        train_start = stock_pool["start_date"].min()
        train_end = date_str
        test_start = date_str
        test_end = date_str   # å–®æ—¥ç‰¹å¾µ

        output_path = OUTPUT_DIR / f"online_ft/{year}/{date_str}.csv"
        output_path.parent.mkdir(parents=True, exist_ok=True)

        generate_alpha158_features(
            symbols,
            start_time=test_start,
            end_time=test_end,
            fit_start_time=train_start,
            fit_end_time=train_end,
            output_path=output_path,
        )
else:
    raise ValueError("âŒ Mode must be 'incremental_pretrain' or 'online_ft'")
