"""
âœ… Step 2ï¼šmerge_features_v5.pyï¼ˆæœ€çµ‚æ•´åˆ + å¯è¦–åŒ–ç‰ˆï¼‰
-------------------------------------------------------
åˆä½µ incremental_pretrain (train/valid) èˆ‡ test ç‰¹å¾µï¼Œ
ç”Ÿæˆçµ±ä¸€æ ¼å¼ parquetï¼Œä¸¦è¼¸å‡ºå®Œæ•´ç¼ºå€¼ / å®Œæ•´ç‡çµ±è¨ˆèˆ‡åœ–è¡¨ã€‚
-------------------------------------------------------
è¼¸å‡ºï¼š
  ğŸ“¦ merged_features.parquet
  ğŸ“Š feature_coverage.csv
  ğŸ“Š symbol_completeness.csv
  ğŸ“Š symbol_date_completeness.parquet
  ğŸ“„ completeness_summary.csv
  ğŸ“ merge_report.txt
  ğŸ–¼ï¸ symbol_completeness_hist.png
  ğŸ–¼ï¸ feature_coverage_bar.png
  ğŸ–¼ï¸ daily_completeness_trend.png
-------------------------------------------------------
"""

import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt

# === è·¯å¾‘è¨­å®š ===
BASE_DIR = Path("./data/qlib_data/day1/generated_datasets")
OUTPUT_FILE = BASE_DIR / "merged_features.parquet"
REPORT_FILE = BASE_DIR / "merge_report.txt"

# === å¹«åŠ©å‡½æ•¸ ===
def load_and_tag_csv(file_path: Path, phase: str, year=None):
    """è®€å–å–®ä¸€ CSV ä¸¦åŠ ä¸Š phase/year æ¨™ç±¤"""
    try:
        df = pd.read_csv(file_path, low_memory=False)
        if "symbol" not in df.columns or "date" not in df.columns:
            print(f"âš ï¸ {file_path.name} ç¼ºå°‘ symbol/date æ¬„ä½ï¼Œè·³é")
            return None
        df["phase"] = phase
        df["year"] = year or str(file_path.stem[:4])
        return df
    except Exception as e:
        print(f"âŒ ç„¡æ³•è®€å– {file_path.name}: {e}")
        return None


def collect_all_phases():
    """æ”¶é›† incremental_pretrain (train/valid) + testï¼ˆonline_ftï¼‰"""
    all_files = []

    # 1) incremental_pretrain: train / valid
    for sub in ["train", "valid"]:
        phase_path = BASE_DIR / f"incremental_pretrain/{sub}"
        for f in sorted(phase_path.glob("*.csv")):
            if f.exists():
                df = load_and_tag_csv(f, phase=sub, year=f.stem)
                if df is not None:
                    all_files.append(df)

    # 2) online_ftï¼Œè¦–ç‚º test
    ft_base = BASE_DIR / "online_ft"
    for year_dir in sorted(ft_base.glob("*")):
        if not year_dir.is_dir():
            continue
        for f in sorted(year_dir.glob("*.csv")):
            df = load_and_tag_csv(f, phase="test", year=year_dir.name)
            if df is not None:
                all_files.append(df)

    return all_files

def detect_global_warmup(merged_df, feature_cols, threshold=0.5, min_days=10):
    """
    åµæ¸¬å…¨é«”è‚¡ç¥¨å‰æœŸ (warm-up) æ‡‰åˆªé™¤çš„ row æ•¸ã€‚
    - threshold: å¹³å‡å®Œæ•´ç‡é–€æª» (ä¾‹å¦‚ <0.5)
    - min_days: é€£çºŒå¹¾å¤©éƒ½é”æ¨™æ‰è¦–ç‚ºç©©å®šæœŸé–‹å§‹
    """
    print("ğŸ“… è¨ˆç®—æ¯æ—¥å…¨é«”è‚¡ç¥¨å¹³å‡å®Œæ•´ç‡ ...")

    daily_completeness = (
        merged_df.groupby("date")[feature_cols]
        .apply(lambda x: 1 - x.isna().mean().mean())
        .rename("avg_completeness")
        .reset_index()
    )

    # æ‰¾å‡ºé€£çºŒ min_days å¤©å®Œæ•´ç‡ >= threshold çš„ç¬¬ä¸€å€‹ä½ç½®
    consecutive_valid = 0
    cutoff_idx = 0
    for i, comp in enumerate(daily_completeness["avg_completeness"]):
        if comp >= threshold:
            consecutive_valid += 1
            if consecutive_valid >= min_days:
                cutoff_idx = i - min_days + 1
                break
        else:
            consecutive_valid = 0

    # å–å¾—æ‡‰åˆªé™¤çš„æ—¥æœŸå€é–“
    if cutoff_idx == 0:
        print("âš ï¸ æ²’æ‰¾åˆ°é€£çºŒé”æ¨™å€æ®µï¼Œè«‹é™ä½ threshold æˆ– min_daysã€‚")
        return 0, daily_completeness

    cutoff_date = daily_completeness.iloc[cutoff_idx]["date"]
    print(f"âœ… å…¨é«”è‚¡ç¥¨å®Œæ•´ç‡ç©©å®šèµ·å§‹æ—¥ï¼š{cutoff_date.strftime('%Y-%m-%d')}, index: {cutoff_idx}"
          f"(threshold={threshold}, min_days={min_days})")

    daily_completeness.to_csv(BASE_DIR / "daily_completeness.csv", index=False)
    print(f"ğŸ“ å·²è¼¸å‡ºæ¯æ—¥å®Œæ•´ç‡ â†’ {BASE_DIR / 'daily_completeness.csv'}")

    return cutoff_date, daily_completeness



# === ä¸»æµç¨‹ ===
def main():
    print("ğŸ§© æ”¶é›† incremental_pretrain / online_ft ç‰¹å¾µæª” ...")
    dfs = collect_all_phases()
    if not dfs:
        print("âŒ æ‰¾ä¸åˆ°ä»»ä½•ç‰¹å¾µæª”ï¼Œè«‹ç¢ºèªè³‡æ–™ç”Ÿæˆæ˜¯å¦å®Œæˆã€‚")
        return

    # === æª¢æŸ¥æ¬„ä½ä¸€è‡´æ€§ ===
    print("ğŸ”§ æª¢æŸ¥æ¬„ä½ä¸€è‡´æ€§ ...")
    common_cols = list(set.intersection(*(set(df.columns) for df in dfs)))
    merged_df = pd.concat([df[common_cols] for df in dfs], ignore_index=True)
    merged_df["date"] = pd.to_datetime(merged_df["date"])
    merged_df = merged_df.sort_values(["symbol", "date"]).reset_index(drop=True)

    # === ç‰¹å¾µæ¬„ä½ ===
    feature_cols = [
        c for c in merged_df.columns
        if c not in ["symbol", "date", "phase", "year"]
        and pd.api.types.is_numeric_dtype(merged_df[c])
    ]
    n_features = len(feature_cols)
    print(f"âœ… æœ‰æ•ˆç‰¹å¾µæ¬„ä½æ•¸ï¼š{n_features}")

    cutoff_date, completeness_df = detect_global_warmup(merged_df, feature_cols, threshold=0.99, min_days=10)
    if cutoff_date:
        merged_df = merged_df[merged_df["date"] >= cutoff_date].reset_index(drop=True)
        merged_df.to_parquet(BASE_DIR / "merged_features.parquet", index=False)
        print(f"âœ… å·²ç§»é™¤å…¨é«” warm-up æ®µï¼ˆ{cutoff_date.date()} ä¹‹å‰ï¼‰ï¼Œåˆä½µï¼Œå…± {len(merged_df):,} ç­† â†’ merged_features.parquet")

    # === æ¬„ä½å®Œæ•´ç‡ (Feature Coverage)
    feature_coverage = (1 - merged_df[feature_cols].isna().mean()).sort_values(ascending=False)
    feature_coverage.to_csv(BASE_DIR / "feature_coverage.csv")

    # === è‚¡ç¥¨å®Œæ•´ç‡ (Symbol completeness)
    symbol_completeness = (
        merged_df.groupby("symbol")[feature_cols]
        .apply(lambda df: 1 - df.isna().mean().mean())
        .rename("symbol_completeness")
        .sort_values(ascending=False)
    )
    symbol_completeness.to_csv(BASE_DIR / "symbol_completeness.csv")

    # === è‚¡ç¥¨ Ã— æ—¥æœŸå®Œæ•´ç‡ (Symbol-Date completeness)
    symbol_date_completeness = (
        merged_df.groupby(["symbol", "date"])[feature_cols]
        .apply(lambda df: 1 - df.isna().mean(axis=1).iloc[0])
        .reset_index(name="completeness_ratio")
    )

    # === æ¯æ—¥æ•´é«”å®Œæ•´ç‡
    daily_completeness = (
        symbol_date_completeness.groupby("date")["completeness_ratio"]
        .mean()
        .rename("avg_completeness")
        .reset_index()
    )

    # === çµ±è¨ˆæ‘˜è¦
    summary = {
        "n_rows": len(merged_df),
        "n_symbols": merged_df["symbol"].nunique(),
        "n_features": n_features,
        "avg_feature_coverage": feature_coverage.mean(),
        "avg_symbol_completeness": symbol_completeness.mean(),
        "min_symbol_completeness": symbol_completeness.min(),
        "max_symbol_completeness": symbol_completeness.max(),
        "avg_daily_completeness": daily_completeness["avg_completeness"].mean(),
    }
    pd.Series(summary).to_csv(BASE_DIR / "completeness_summary.csv")

    # === å ±å‘Šè¼¸å‡º ===
    with open(REPORT_FILE, "w", encoding="utf-8") as f:
        f.write("ğŸ“˜ Merge Features Report (v5 å¯è¦–åŒ–ç‰ˆ)\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"è³‡æ–™ç­†æ•¸ï¼š{summary['n_rows']:,}\n")
        f.write(f"è‚¡ç¥¨æ•¸é‡ï¼š{summary['n_symbols']}\n")
        f.write(f"ç‰¹å¾µæ•¸é‡ï¼š{summary['n_features']}\n\n")
        f.write(f"å¹³å‡ç‰¹å¾µè¦†è“‹ç‡ï¼š{summary['avg_feature_coverage']:.4f}\n")
        f.write(f"å¹³å‡è‚¡ç¥¨å®Œæ•´ç‡ï¼š{summary['avg_symbol_completeness']:.4f}\n")
        f.write(f"æ¯æ—¥å¹³å‡å®Œæ•´ç‡ï¼š{summary['avg_daily_completeness']:.4f}\n")
        f.write(f"æœ€å°è‚¡ç¥¨å®Œæ•´ç‡ï¼š{summary['min_symbol_completeness']:.4f}\n")
        f.write(f"æœ€å¤§è‚¡ç¥¨å®Œæ•´ç‡ï¼š{summary['max_symbol_completeness']:.4f}\n")

    # === å¯è¦–åŒ–éƒ¨åˆ† ===
    print("ğŸ¨ ç”¢ç”Ÿåœ–è¡¨ ...")

    # 1ï¸âƒ£ æ¯æª”è‚¡ç¥¨å®Œæ•´ç‡åˆ†ä½ˆ
    plt.figure(figsize=(8, 5))
    plt.hist(symbol_completeness, bins=50, edgecolor='black')
    plt.title("Symbol Completeness Distribution")
    plt.xlabel("Completeness Ratio")
    plt.ylabel("Number of Stocks")
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(BASE_DIR / "symbol_completeness_hist.png")
    plt.close()

    # 2ï¸âƒ£ ç‰¹å¾µè¦†è“‹ç‡å‰å¾Œ 20 å
    plt.figure(figsize=(10, 6))
    top_features = pd.concat([feature_coverage.head(10), feature_coverage.tail(10)])
    top_features.plot(kind='barh', color='steelblue', edgecolor='black')
    plt.title("Feature Coverage (Top & Bottom 10)")
    plt.xlabel("Coverage Ratio")
    plt.ylabel("Feature Name")
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(BASE_DIR / "feature_coverage_bar.png")
    plt.close()

    # 3ï¸âƒ£ æ¯æ—¥æ•´é«”å®Œæ•´ç‡è¶¨å‹¢
    plt.figure(figsize=(10, 5))
    plt.plot(daily_completeness["date"], daily_completeness["avg_completeness"], lw=1.5)
    plt.title("Daily Average Completeness Trend")
    plt.xlabel("Date")
    plt.ylabel("Average Completeness")
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(BASE_DIR / "daily_completeness_trend.png")
    plt.close()

    # === è¼¸å‡ºæ‘˜è¦ ===
    print("ğŸ“Š çµ±è¨ˆèˆ‡åœ–è¡¨å·²è¼¸å‡ºè‡³è³‡æ–™å¤¾ï¼š")
    for file in [
        "feature_coverage.csv", "symbol_completeness.csv",
        "symbol_date_completeness.parquet", "completeness_summary.csv",
        "symbol_completeness_hist.png", "feature_coverage_bar.png", "daily_completeness_trend.png"
    ]:
        print(" â””â”€", BASE_DIR / file)


if __name__ == "__main__":
    main()
