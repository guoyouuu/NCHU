{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Sf9QKr0UbSTL"
      },
      "outputs": [],
      "source": [
        "def get_mnist():\n",
        "    # The code to download the mnist data original came from\n",
        "    # https://cntk.ai/pythondocs/CNTK_103A_MNIST_DataLoader.html\n",
        "\n",
        "    import gzip\n",
        "    import numpy as np\n",
        "    import os\n",
        "    import struct\n",
        "\n",
        "    from urllib.request import urlretrieve\n",
        "\n",
        "    def load_data(src, num_samples):\n",
        "        print(\"Downloading \" + src)\n",
        "        gzfname, h = urlretrieve(src, \"./delete.me\")\n",
        "        print(\"Done.\")\n",
        "        try:\n",
        "            with gzip.open(gzfname) as gz:\n",
        "                n = struct.unpack(\"I\", gz.read(4))\n",
        "                # Read magic number.\n",
        "                if n[0] != 0x3080000:\n",
        "                    raise Exception(\"Invalid file: unexpected magic number.\")\n",
        "                # Read number of entries.\n",
        "                n = struct.unpack(\">I\", gz.read(4))[0]\n",
        "                if n != num_samples:\n",
        "                    raise Exception(\n",
        "                        \"Invalid file: expected {0} entries.\".format(num_samples)\n",
        "                    )\n",
        "                crow = struct.unpack(\">I\", gz.read(4))[0]\n",
        "                ccol = struct.unpack(\">I\", gz.read(4))[0]\n",
        "                if crow != 28 or ccol != 28:\n",
        "                    raise Exception(\n",
        "                        \"Invalid file: expected 28 rows/cols per image.\"\n",
        "                    )\n",
        "                # Read data.\n",
        "                res = np.frombuffer(\n",
        "                    gz.read(num_samples * crow * ccol), dtype=np.uint8\n",
        "                )\n",
        "        finally:\n",
        "            os.remove(gzfname)\n",
        "        return res.reshape((num_samples, crow, ccol)) / 256\n",
        "\n",
        "\n",
        "    def load_labels(src, num_samples):\n",
        "        print(\"Downloading \" + src)\n",
        "        gzfname, h = urlretrieve(src, \"./delete.me\")\n",
        "        print(\"Done.\")\n",
        "        try:\n",
        "            with gzip.open(gzfname) as gz:\n",
        "                n = struct.unpack(\"I\", gz.read(4))\n",
        "                # Read magic number.\n",
        "                if n[0] != 0x1080000:\n",
        "                    raise Exception(\"Invalid file: unexpected magic number.\")\n",
        "                # Read number of entries.\n",
        "                n = struct.unpack(\">I\", gz.read(4))\n",
        "                if n[0] != num_samples:\n",
        "                    raise Exception(\n",
        "                        \"Invalid file: expected {0} rows.\".format(num_samples)\n",
        "                    )\n",
        "                # Read labels.\n",
        "                res = np.frombuffer(gz.read(num_samples), dtype=np.uint8)\n",
        "        finally:\n",
        "            os.remove(gzfname)\n",
        "        return res.reshape((num_samples))\n",
        "\n",
        "\n",
        "    def try_download(data_source, label_source, num_samples):\n",
        "        data = load_data(data_source, num_samples)\n",
        "        labels = load_labels(label_source, num_samples)\n",
        "        return data, labels\n",
        "\n",
        "    # Not sure why, but yann lecun's website does no longer support\n",
        "    # simple downloader. (e.g. urlretrieve and wget fail, while curl work)\n",
        "    # Since not everyone has linux, use a mirror from uni server.\n",
        "    #     server = 'http://yann.lecun.com/exdb/mnist'\n",
        "    server = 'https://raw.githubusercontent.com/fgnt/mnist/master'\n",
        "\n",
        "    # URLs for the train image and label data\n",
        "    url_train_image = f'{server}/train-images-idx3-ubyte.gz'\n",
        "    url_train_labels = f'{server}/train-labels-idx1-ubyte.gz'\n",
        "    num_train_samples = 60000\n",
        "\n",
        "    print(\"Downloading train data\")\n",
        "    train_features, train_labels = try_download(url_train_image, url_train_labels, num_train_samples)\n",
        "\n",
        "    # URLs for the test image and label data\n",
        "    url_test_image = f'{server}/t10k-images-idx3-ubyte.gz'\n",
        "    url_test_labels = f'{server}/t10k-labels-idx1-ubyte.gz'\n",
        "    num_test_samples = 10000\n",
        "\n",
        "    print(\"Downloading test data\")\n",
        "    test_features, test_labels = try_download(url_test_image, url_test_labels, num_test_samples)\n",
        "\n",
        "    return train_features, train_labels, test_features, test_labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# —— get_mnist() 已經在上方定義好 —— #\n",
        "# 直接取資料\n",
        "train_features, train_labels, test_features, test_labels = get_mnist()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. 轉 tensor 建立 DataLoader（VAE 自監督，label 不會用到）\n",
        "train_x = torch.tensor(train_features, dtype=torch.float32)\n",
        "test_x  = torch.tensor(test_features, dtype=torch.float32)\n",
        "train_dataset = TensorDataset(train_x, train_x)\n",
        "test_dataset  = TensorDataset(test_x, test_x)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# 2. VAE 模型\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, z_dim=20, h_dim=400):\n",
        "        super().__init__()\n",
        "        self.fc1  = nn.Linear(28 * 28, h_dim)\n",
        "        self.fc21 = nn.Linear(h_dim, z_dim)  # mu\n",
        "        self.fc22 = nn.Linear(h_dim, z_dim)  # logvar\n",
        "        self.fc3  = nn.Linear(z_dim, h_dim)\n",
        "        self.fc4  = nn.Linear(h_dim, 28 * 28)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = torch.relu(self.fc1(x))\n",
        "        return self.fc21(h), self.fc22(h)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = torch.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h))  # 配合 BCE 使用 sigmoid\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_hat = self.decode(z)\n",
        "        return x_hat, mu, logvar\n",
        "\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    x = x.view(-1, 28 * 28)\n",
        "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KL  = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KL, BCE, KL\n",
        "\n",
        "# 3. 裝置、隨機種子、模型與優化器\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(42)\n",
        "model = VAE(z_dim=32, h_dim=512).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 4. 訓練與測試\n",
        "def train_epoch(epoch):\n",
        "    model.train()\n",
        "    total, total_bce, total_kl = 0.0, 0.0, 0.0\n",
        "\n",
        "    for b, (x, _) in enumerate(train_loader):\n",
        "        # 批次讀資料，搬到 CPU/GPU\n",
        "        x = x.to(device)\n",
        "        # 清掉上一步的梯度，避免累加污染\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_hat, mu, logvar = model(x)\n",
        "        loss, bce, kl = vae_loss(recon_x=x_hat, x=x, mu=mu, logvar=logvar)\n",
        "\n",
        "        # 反傳、更新參數\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "        total_bce += bce.item()\n",
        "        total_kl += kl.item()\n",
        "\n",
        "        if b % 100 == 0:\n",
        "            print(f\"Epoch {epoch} | {b*len(x)}/{len(train_loader.dataset)} \"\n",
        "                  f\"Loss/sample: {loss.item()/len(x):.4f}\")\n",
        "\n",
        "    N = len(train_loader.dataset)\n",
        "    print(f\"[Train] Epoch {epoch} - avg Loss: {total/N:.4f} | BCE: {total_bce/N:.4f} | KL: {total_kl/N:.4f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(epoch):\n",
        "    model.eval()\n",
        "    total, total_bce, total_kl = 0.0, 0.0, 0.0\n",
        "\n",
        "    for x, _ in test_loader:\n",
        "        x = x.to(device)\n",
        "\n",
        "        x_hat, mu, logvar = model(x)\n",
        "        loss, bce, kl = vae_loss(recon_x=x_hat, x=x, mu=mu, logvar=logvar)\n",
        "\n",
        "        total += loss.item()\n",
        "        total_bce += bce.item()\n",
        "        total_kl += kl.item()\n",
        "\n",
        "    N = len(test_loader.dataset)\n",
        "    print(f\"[Test ] Epoch {epoch} - avg Loss: {total/N:.4f} | BCE: {total_bce/N:.4f} | KL: {total_kl/N:.4f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def save_reconstruction(n=8):\n",
        "    model.eval()\n",
        "\n",
        "    # 視覺化\n",
        "    x_vis, _ = next(iter(test_loader))\n",
        "    x_vis = x_vis.to(device)\n",
        "    x_hat, _, _ = model(x_vis)\n",
        "\n",
        "    imgs = x_vis[:n].cpu().numpy()\n",
        "    recons = x_hat[:n].view(-1, 28, 28).cpu().numpy()\n",
        "\n",
        "    fig, axes = plt.subplots(2, n, figsize=(10, 3))\n",
        "    for i in range(n):\n",
        "        axes[0, i].imshow(imgs[i], cmap=\"gray\");   axes[0, i].axis(\"off\")\n",
        "        axes[1, i].imshow(recons[i], cmap=\"gray\"); axes[1, i].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # 檔名\n",
        "    save_path = f\"reconstruction.png\"\n",
        "    plt.savefig(save_path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    print(f\"已儲存重建圖檔 {save_path}\")\n",
        "\n",
        "# 5. Execute\n",
        "epochs = 5\n",
        "for ep in range(1, epochs+1):\n",
        "    train_epoch(ep)\n",
        "    evaluate(ep)\n",
        "\n",
        "print(\"訓練完成！\")\n",
        "save_reconstruction()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1lz7c3tmrPI",
        "outputId": "b83ad3ee-683e-4c2e-a140-04bf1ecab9f2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading train data\n",
            "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz\n",
            "Done.\n",
            "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n",
            "Done.\n",
            "Downloading test data\n",
            "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/t10k-images-idx3-ubyte.gz\n",
            "Done.\n",
            "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/t10k-labels-idx1-ubyte.gz\n",
            "Done.\n",
            "Epoch 1 | 0/60000 Loss/sample: 549.5739\n",
            "Epoch 1 | 12800/60000 Loss/sample: 179.3973\n",
            "Epoch 1 | 25600/60000 Loss/sample: 160.3232\n",
            "Epoch 1 | 38400/60000 Loss/sample: 139.9454\n",
            "Epoch 1 | 51200/60000 Loss/sample: 137.0203\n",
            "[Train] Epoch 1 - avg Loss: 162.6857 | BCE: 146.7369 | KL: 15.9488\n",
            "[Test ] Epoch 1 - avg Loss: 128.8769 | BCE: 106.2950 | KL: 22.5818\n",
            "Epoch 2 | 0/60000 Loss/sample: 130.5891\n",
            "Epoch 2 | 12800/60000 Loss/sample: 121.6951\n",
            "Epoch 2 | 25600/60000 Loss/sample: 123.7571\n",
            "Epoch 2 | 38400/60000 Loss/sample: 120.4309\n",
            "Epoch 2 | 51200/60000 Loss/sample: 115.6537\n",
            "[Train] Epoch 2 - avg Loss: 122.4872 | BCE: 99.0211 | KL: 23.4660\n",
            "[Test ] Epoch 2 - avg Loss: 116.5504 | BCE: 92.2834 | KL: 24.2670\n",
            "Epoch 3 | 0/60000 Loss/sample: 119.5572\n",
            "Epoch 3 | 12800/60000 Loss/sample: 111.9704\n",
            "Epoch 3 | 25600/60000 Loss/sample: 116.0371\n",
            "Epoch 3 | 38400/60000 Loss/sample: 116.7814\n",
            "Epoch 3 | 51200/60000 Loss/sample: 111.7031\n",
            "[Train] Epoch 3 - avg Loss: 114.8614 | BCE: 89.7165 | KL: 25.1450\n",
            "[Test ] Epoch 3 - avg Loss: 112.0615 | BCE: 86.3138 | KL: 25.7476\n",
            "Epoch 4 | 0/60000 Loss/sample: 112.1945\n",
            "Epoch 4 | 12800/60000 Loss/sample: 113.5926\n",
            "Epoch 4 | 25600/60000 Loss/sample: 112.9213\n",
            "Epoch 4 | 38400/60000 Loss/sample: 114.2694\n",
            "Epoch 4 | 51200/60000 Loss/sample: 112.1491\n",
            "[Train] Epoch 4 - avg Loss: 111.7227 | BCE: 85.9359 | KL: 25.7868\n",
            "[Test ] Epoch 4 - avg Loss: 109.8140 | BCE: 84.1375 | KL: 25.6765\n",
            "Epoch 5 | 0/60000 Loss/sample: 112.7025\n",
            "Epoch 5 | 12800/60000 Loss/sample: 115.8331\n",
            "Epoch 5 | 25600/60000 Loss/sample: 109.2877\n",
            "Epoch 5 | 38400/60000 Loss/sample: 109.2887\n",
            "Epoch 5 | 51200/60000 Loss/sample: 110.8543\n",
            "[Train] Epoch 5 - avg Loss: 109.8694 | BCE: 83.9100 | KL: 25.9594\n",
            "[Test ] Epoch 5 - avg Loss: 108.4274 | BCE: 82.4822 | KL: 25.9451\n",
            "訓練完成！\n",
            "已儲存重建圖檔 reconstruction.png\n"
          ]
        }
      ]
    }
  ]
}